[aura]
log-level = info

# Path to the location of the offline PyPI mirror
# This option can be safely disabled if you don't have a local mirror, some advanced features require this
mirror = /var/pypi_mirror/pypi/web/

# Path to the yara rules for scanning data blobs
# See example_rules.yara for documentation on format and examples
yara-rules = rules.yara

# Path to the semantic rules used by python code analyzer
semantic-rules = signatures.json

# This file is needed for typosquatting detections
pypi_stats = pypi_stats.json

# Default minimum score for outputting scan hits
# This can be overridden by a specific output type
min-score = 10

# You can enable/disable forking for async processing of files here
# Async processing is using the python's multiprocessing module
# It has a much bigger performance but it is not very debugger friendly when developing new plugins
# It is possible that some 3rd party plugins might require synchronous processing due to data pipelines
async = false

# Limit heap size of the process
# 4G
rlimit-memory = 4294967296

# Limit maximum file size the framework can create
# This is also used as a limit when unpacking archive content to prevent for example zip bombs
# 4G
rlimit-fsize = 4294967296

# You can limit the stack (python frames) recursion here
# python-recursion-limit = 150
# Aura recursively unpack archives, this specifies the maximum depth of recursion
max-depth = 3

# Define a maximum number of iterations for a visitor
# A new iteration is performed over the AST tree each time a property or a node of the AST tree is modified
# These iterations are performed until the AST tree converges (e.g. no more modifications are performed) or a maximum number of iterations has been reached
max-ast-iterations = 500

# This is a prevention against infinite traversals/loops in the AST tree, which puts a hard limit on queue and discards any new node traversals above the limit
# In case there is a bug, AST tree could be rewritten in a way that creates loops
# In some rare cases, the source code could just be extremely big which prolongs the processing a lot, especially the taint analysis
max-ast-queue-size = 10000

# Set preferred output format for cli commands that supports it
# Supported formats: text, json
output-format = json

# If defined, a dedicated log file for exceptions and errors would be created
error-log = "aura_errors.log"

# Define the threshold of shanon entropy for strings to be reported
shanon_entropy = 5.0

[api_tokens]
# You can define api tokens for integrations here
# Another option is to configure them via an environment variable using:
# AURA_TOKENNAME_TOKEN = "your token"
# List of supported tokens, just uncomment and replace the following lines:

#librariesio = "insert_token_here"

[tags]
# Filter results that contain only the specified tags
# Results can also be excluded using "!" to prefix a tag
# This list would be used for default tag filtering and can be optionally overriden via cli parameter(s) -t
#!test-code


[score]
# Score assigned when a package contain a suspicious file inside such as python bytecode (*.pyc)
contain-suspicious-file = 5

# Score assigned when a package contain a sensitive file inside such as accidently including .pypirc
contain-sensitive-file = 100

[interpreters]
# Configure python interpreters for parsing AST code
# `python2` must point to the py2.7+ version (versions under 2.7 are not supported but might work)
# `python3` must point to the py3.6+ or ideally py3.7+ due to compatibility
# All other interpreters are optional, AST parsing will try them in the defined order
python2 = python2
python3 = python3
